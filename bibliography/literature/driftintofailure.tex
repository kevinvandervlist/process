\chapter{Drift Into Failure\cite{dekker2011drift}}
\section{Failure is always an option}
Episteological space: Identify things that are broken, by dissecting structures.
Rational choice theory: people check all scenarios and pick the one that maximises their utility; e.g. profitability. 
An imperfection in hindsight is a non-optional choice. 
Assumes rational people with perfect information. Training or otherwise enforcing will boost results. 

Another issue is that the resources are limited (usually time and intellect).
Complete rational behaviour is impossible, input domain is infinite.
Local fine results can cause global failure because of this lack of oversight.

Problem now: Try to find the faulty component, but not what was the cause of that faulty component.

People think they can find broken part, which in turn explains the reasons behind failure. 
This is not true, because all (technological) systems are becoming so complex, we can only understand them in isolation. 
Finding the broken piece therefore is not enough to structurally fix processes. 
Core message of the book: 
\begin{citation}
the complexity of what society and commerce can give rise to today is not matched by the theories we have that can explain why such things go wrong.
\end{citation}

Locally acceptable decisions have ripple effect on global state.
Accepted decisions in certain local domain can also make sense, but decrease in quality over time, as environment changes. 
This results in a slow move to an unstable system, e.g. drifting into failure.
Drifting occurs because of conflicting forces (e.g. profit vs safety), and in small steps (sliding slope, small deviations at a time, introducing new norms).
Third: initial choices have larger effect then choices later on.
Finally: complex systems draw on regulation defence around it (e.g. ignorance of safety on aspect X implicitly lowers its quality).

Failure trajectory is hard to see when in place, but easy in hindsight. 
The drift cannot be seen while on the inside of a complex system.
In the end, drift into failure also depends on what information you relate to the complex system. Introducing more sources can for example produce a different, but just as good story. 
Interpretation is sadly the subjective key.

\section{Features of drift}
``western'' idea: Failures in processes can be explained by other failures (e.g. broken part is caused by broken maintenance protocol is broken by management strategy).
This is kind of a static way of thinking, and does not provide \emph{why} this happens, what the underlying reason is (e.g. profit optimisation, low margins for divergence of plannings).

In hindsight these are clear, but in motion things can look totally fine. 
Everything is a trade off, and these might make perfect sense at the time.
Decisions made during a drift into failure might be ludicrous in hindsight, but then you can analyse their consequences. 
A lot of drifting usually is undiscovered when not resulting into failure.

Five characteristic concepts for drift are a) scarcity and competition, b) small steps, c) Sensitive dependence on small steps, d) unruly technology and e) contribution of protective structure.

Scarcity and competition pressure a system, because ``the winner takes all''. Make a mistake, and you are punished (e.g. economically). 
This results in (among others) rushing of safety regulations. 
The trade off at that time is simple: the system functions perfectly fine, so this minor productivity increment is fine.

In a small step process, this drifting occurs. 
Instead of a single, large change, numerous small changes are applied.

Initial condition sensitivity (butterfly effect).
Certification and acceptance regulations are generally based on a starting position, and hardly seen as a lifetime process. 
Changing environments can be a reason to change the basis of a certification, instead of blindly trusting it in any new situation.

Unruly technology (disorderly, not amenable to control) is about things like technology that are put into a practical context. 
The interaction with their environment is not completely understood, or can be hard to manage. 
As an example: theoretical wear might occur in a time span of 10 months, while in practice it is 6.

Finally, contribution of the protective structure means that protection mechanisms can constitute to a failure. 
An organisation in place to monitor and act on deficiencies might provide such support to the structure it protects that the intensity of early warning signals might be diminished. 
The structure put in place to protect might end up supporting the transition into failure.
As an example, this can happen when negotiating about a standard repair procedure interval.

For the story of drifting into failure, processes are interesting as well. 
If an employee is always juggling to use 3 items with 2 hands, this is an indicator of risk as well.
Trying to improve the process will be useful, but it is something that hardly will be written down in test reports etc.

\section{legacy of newton and descartes}
Galileo: don't trust subjective data, but only measurements and quantification.
Newton: Laws of nature so prediction and preemptive action is an option.
Descartes: environmental interaction can be reduced to mathematics, so he inferred you could end up with an absolute, proven truth.
The above is done on reductionism assumption, you take an element, and study it in isolation. 
Issue: Not a complex system anymore.

However, failures are more then the proximal cause. 
The organisation must be checked as well (swiss chees: latent causes).
Shortcomings of such systems occur when looking at the dynamics of a system. 
It works great in linear failures (like colombia crash), but it cannot grab the total issue if there are a lot of individual connections within a system. 
A system as a whole is a combinatory explosion of small decisions (under pressure) that contribute to the drift into failure.

Complex systems can also be self-strengthening.
Safety measures put in place for protection come at the cost of extra actions and dependence on other protocols or hardware. 
This results in the combinatioral explosion, and can cause failures without having an explicitly broken part in a system.

\section{The search for the broken component}
This search usually is taking the reductionist approach.
Each system is taken apart until it is understood (e.g. Human error $\rightarrow$ memory failure $\rightarrow$ \ldots).
This removes the individual elements from its usual environment, and therefore influences its behaviour. 
People react differently in a group compared to individually.

Newtonian assumes that having perfect knowledge about behaviour and start condition results in perfect predictability. 
Unforeseen things cannot happen, and therefore are ``broken parts''. 
If you believe this, a ``broken part'' can be found in any mistake, but this usually is not the cause of it.

\section{Theorizing Drift}
\begin{quote}
The gap between risk-in-the-world and risk-as-perceived grows because somebody, somewhere in the system, is not detecting things that could be detected. 
This represents broken components that can be tracked down and fixed or replaced.
\end{quote}

Risk can also be conceptualised as energy. 
According to this theory, layers or in/output controlling protection can be applied, in which buildup of ``risk'' is controlled or contained. 

Operational people do not define safety in terms of risk management / error avoidance. Four other things do, which became the heart of high reliability theory. 
Leadership safety objectives: Everyone should be part of same safety mindset, and everything including production is less important.
The second is the need for redundancy. Introduce duplication and overlap to create a reliable system.
Third there is decentralisation, culture and continuity. Active searching for security related improvements and the authority to introduce such things without ``asking higherups''. 
Be active, don't wait for regulations.
Finally, the fourth element is organisational learning: Incremental learning through trial and error. 
The goal is that small dangers should also be prevented.
In short: operational views should trump organisational views,

For an insider it is hard to respond to warning signals, they cannot see the whole system at once because of complexity. 
They hardly cannot be recognised and if the mind does not believe the gravity of the signals, the mind cannot make sense of them, leaving the warning signals unnoticed. 
In hindsight, the system workings might became lucid, and side effects are visible. 
This increases awareness, thus making things easier.
One of the important causes: trade offs have to be made because of scarcity stuff (competition, pressure, \ldots). 
As an example: faster, better, cheaper cannot all three at once be satisfied because of their conflicting nature.

macro-micro connection: play of forces on different levels. Organisation on one hand (macro) creates norms, targets etc. and to get things done. 
micro: response of individual employee, usually trying to satisfy those macro goals, which tend to be conflicting.
This system can reinforce itself, because drifting is gradually. 
A trade off negative for safety is ``proved'' successful because of successful execution of complex system.
Choices therefore make sense at the time.

(Long) quote about cycles:
\begin{itemize}
\item \emph{Beginning the construction of risk: a redundant system}. The starting point for the
safety-critical activity is the belief that safety is assured and risk is under
control. Redundancies, the presence of extraordinary competence, or the
use of proven technology can all add to the impression that nothing will
go wrong. There is a senior surgeon who watches the operation through
a laparoscope from a distance.
\item \emph{Signals of potential danger}. Actual use or operation shows a deviation from
what is expected. This creates some uncertainty, and can indicate a threat
to safety, thus challenging the original construction of risk.
\item \emph{Official act acknowledging escalated risk}. Evidence is shown to relevant people,
a meeting may be called.
\item \emph{Review of the evidence}. After the operation, discussions may ensue about
who did what and how well things went. This is not necessarily the case
after all operations; in fact, it could be the kind of standardized debrief
that is hardly institutionalized in medicine yet (see Chapter 6).
\item \emph{Official act indicating the normalization of deviance: accepting risk}. The escalated
risk can get rationalized or normalized as in-family, as expected. It may be
argued that redundancy was assured at multiple levels in the system. And,
after all, the technology itself has undergone various stages of testing and
revision before being fielded. All these factors contribute to a conclusion
that any risk is duly assessed, and under control.
\item \emph{Continued operation}. The technology will be used again in the next flight,
because nothing went wrong, and a review of the risks has revealed that
everything is under control.
\end{itemize}
Each cycle confirms that organisation is having control, e.g. process is validated.

Pressures of safety structures of a system (validation, double check, \ldots) can also be the cause of mistakes. 
Ironically, this means that a safety structure can actually cause harm.
Around the same thing happens with ad-hoc decision making. 
On the short term, benefits might be reaped (which is the direct feedback) but long term dangers and deviation of the rest of the organisation is ignored, resulting in drift.

Organisations also have a hard time achieving a clear understanding of risks. 
People of varying interest and vision come and go, each with leveraging different levels of effort for an organisation.
Next to this, global or hierarchical division leads up to fragmentation of information, which does not allow a complete understanding of all information, enforcing the local sense-making behaviour instead of its global counterpart.

Core of control theory in this context: boundaries are on the move, and inserting (safety) measures to create pressure on opposite sites, discarding their use or even inverting it.

\section{What is complexity and systems thinking?}
Redundancy can increase risk because the communication between two redundant systems increases complexity, sometimes making it a complex system. 
As an example: Aircraft collision detection software old + new worked perfectly fine, but faulty conditions were seen because of communication issues between the two.

When analysing complex systems, make sure you take outside view, and reflect on every event.
If analysing things, for example progress, take a broad perspective.
Is progress less deaths in aviation domain? Which deaths; of plane crashes? 
Increased war-related activity near oil wells? 
Global warming?
Such an analysis cannot be broken up in individual parts (e.g. newtonian-cartesian stuff). 
You have to look at the complete interactions to see the whole picture, not just an individual element.

Alternative to newtonian-cartesian theory: \textbf{Complex Systems Theory}.
Summary (quoted again):
\begin{itemize}
\item Complex systems are open systems -- open to influences from the environment in which they operate and influencing that environment in return. 
Such openness means that it is difficult to frame the boundaries around a system of interest.
\item In a complex system, each component is ignorant of the behavior of the system as a whole, and doesn’t know the full effects of its actions either. 
Components respond locally to information presented by them there and then. 
Complexity arises from the huge, multiplied webs of relationships and interactions that result from these local actions.
\item Complexity is a feature of the system, not of components inside it.
The knowledge of each component is limited and local, and there is no component that possesses enough capacity to represent the complexity of the entire system in that component itself. 
This is why the behaviour of the system cannot be reduced to the behavior of the constituent components, but only characterized on the basis of the multitude of ever-changing relationships between them.
\item Complex systems operate under conditions far from equilibrium. Inputs
need to be made the whole time by its components in order to keep it
functioning. Without that constant flow of actions, of inputs, it cannot
survive in a changing environment. The performance of complex systems
is typically optimized at the edge of chaos, just before system behavior
will become unrecognizably turbulent.
\item Complex systems have a history, a path-dependence. 
Their past is co-responsible for their present behavior, and descriptions of complexity have to take history into account.
\item Interactions in complex systems are non-linear. 
That means that there is an asymmetry between, for example, input and output, and that small events can produce large results. 
The existence of feedback loops means that complex systems can contain multipliers (where more of one means more of the other, in turn leading to more of one, and so forth) and butterfly effects.
\end{itemize}

This theory also has the ``butterfly effect'', in which a small action can have a larger, asymmetric result.
For example, favourite person is out of the running, affecting the morale of others, leading towards a lost battle.

Adaption is crucial in complex systems. 
If the resistance for achieving results becomes to high (e.g. results are hard to achieve), alternatives are being sought. 
A complex systems is therefore a kind of self-organising system.
This also means you cannot ``take a single component'' out of a complex systems and let it collapse. 
Because of this self-organising ability, it just runs on.

There also is a ``historic awareness'' in complex systems. 
Current behaviour is based on experiences and rules from the past. 
The inverse is that decisions made now can have a huge echo into the future.

Emergent properties of complex systems do not have to be showing characteristics of its individual components.
This is an embodiment of a complex system being more then ``the sum of its components''.
In dynamic complex systems, this results in self-organising complexity. 
Nothing has to be in charge, or overruling other components, but together they can come up with a resulting, organised complex system.
A risk introduced by this behaviour is that humans are not equal systems. 
They will shape their local environment though, possibly resulting in a situation where faults emerge from the created system. 
This is very unpredictable because of all the changing individual components though.

Behaviour of a complex system can also change because of a simple thing, a tipping point.
This basically means a small thing, heralding a huge set of changes (e.g. one accident too much -- the whole neighbourhood moves away.)

Complex systems (e.g. organisations as well) tend to settle at ``the edge of chaos''. 
On the one hand, try to do better then any competitive force (results), but do it with the least amount of effort (low cost). 

\chapter{Managing the complexity of drift}
To increase resilience, diversity should preferably be utilised.
This means that when problems arise, not the same solution has to be applied every time, decreasing the chance of introducing a new point of failure (standardised ``wrong'' solution).
Entrepreneurial individuals are of great value in such situations.
In short, complex systems should strive towards diversity.

In order to steer the drift, the five features of drift can be used as levers of control.
Remember: Five characteristic concepts for drift are a) scarcity and competition, b) small steps, c) Sensitive dependence on small steps, d) unruly technology and e) contribution of protective structure.

Resource Scarcity and competition can be leveraged when people are put in a position to question the current strategy of an organisation. 
If the current system is hunting after the last minor efficiency improvements, it might just cut the last bit of slack the system has in case of a fault. 
Be critical, and make sure you can control the drift.

Sensitive dependence and small steps can be utilised by enforcing users to critically reflect such small steps. 
After all, local benefits do not mean global benefits per se.
Since drift is characterised by such small changes, this is the perfect opportunity to do.
As a bonus, people are less sensitive to reflect small changes then big, organisational changing ones.

Unruly technology: do not only question the behaviour of the technology in question, but also your understanding. 
Why did it behave in X while I suspected Y?
Combining this with diversity can make the unruly technology domain grow into a ``landscape of learning''.

Contribution of the protective structure: make sure you have multiple stories, multiple angles of analysing a complex structure. 
To enable this, a ``co-evolving'' organisation might be of use (e.g. try to come up with alternatives, improvements, \ldots $\rightarrow$ R\&D dept.).

In hindsight a decision looks like it determines an outcome. 
At the time, you cannot know (both uncertainty \& inside view). 

Issues with the aftermath of a failure of a complex system:
\begin{itemize}
\item In a complex system, there is no objective way to determine whose view is right and whose view is wrong, since the agents effectively live in different environments. 
This means that there is never one “true” story of what happened. 
That people have different accounts of what happened in the aftermath of failure should not be seen as somebody being right and somebody being wrong, or as somebody wanting to dodge or fudge the “truth.” 
In fact, if somebody claims to have the true story of what happened, it turns everybody else into a liar.
\item A complex system should aim for diversity, and respect otherness and difference as values in themselves. 
Diversity of narratives can be seen as an enormous source of resilience in complex systems (it is when it comes to biodiversity, after all), not as a weakness. 
The more angles, the more there can be to learn.
\item In a complex system, we should gather as much information on the issue as possible. Of course, complexity makes it impossible to gather “all” the information, or for us to even know how much information we have gathered. 
But knowing less almost always puts us at a disadvantage when it comes to rendering a verdict about whether certain actions taken inside a complex system are to be seen as right or wrong.
\item In a complex system, we should consider as many of the possible consequences of any judgment in the aftermath of failure, even though it is of course impossible to consider all the consequences. 
This impossibility does not discharge people of their ethical responsibility to try, particularly not if they are in a position of power; where decisions get made and sustained about the fate of people involved, or about the final word on a story of failure.
\item In a complex system, we should make sure that it is possible to revise any judgement in the wake of failure when as it becomes clear that it has flaws.
Of course, the conditions of a complex system are irreversible, which means that even when a judgment is reversed, some of its consequences (psychological, practical) will probably remain irreversible.
\end{itemize}
